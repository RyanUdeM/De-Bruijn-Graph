{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanUdeM/De-Bruijn-Graph/blob/master/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9by_jAM_CBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "644hLc5Sc0P3",
        "colab_type": "text"
      },
      "source": [
        "## Q1.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSkONVM9HNYg",
        "colab_type": "code",
        "outputId": "955d08cb-4212-4745-8dce-30988ded9f57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Q1.1\n",
        "seed = 0\n",
        "n_hidden = 2\n",
        "dims = [10,2]\n",
        "hidden_dims=(512, 256)\n",
        "\n",
        "if seed is not None:\n",
        "    np.random.seed(seed)\n",
        "\n",
        "weights = {}\n",
        "# self.weights is a dictionary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "all_dims = [dims[0]] + list(hidden_dims) + [dims[1]]  ### all_dims = [input_dim, hidden1_dim, hidden2_dim, output_dim]\n",
        "for layer_n in range(1, n_hidden + 2):\n",
        "    # WRITE CODE HERE #############\n",
        "    random_matrix = np.random.rand(all_dims[layer_n -1], all_dims[layer_n]) * 2 -1 ### random matrix [-1,1]\n",
        "    W = random_matrix /np.sqrt(layer_n) \n",
        "    weights[f\"W{layer_n}\"] = W\n",
        "\n",
        "    # ######################\n",
        "    weights[f\"b{layer_n}\"] = np.zeros((1, all_dims[layer_n]))  ### b is a row vector has dimension of (1, hidden_dim)\n",
        "\n",
        "print(all_dims)\n",
        "print(\"b2 dim = \", weights['b2'].shape)\n",
        "weights['b2'][0].shape\n",
        "weights['W2'].shape"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[10, 512, 256, 2]\n",
            "b2 dim =  (1, 256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(512, 256)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVgaSE-xTw4a",
        "colab_type": "code",
        "outputId": "bcad78b4-c2f7-491e-95d7-cbd2ec899d18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "np.random.rand(2,3)*2 -1"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.7863354 , -0.81775322, -0.25532018],\n",
              "       [ 0.3572054 , -0.30865997, -0.11289668]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsJF5Ptwcmlo",
        "colab_type": "text"
      },
      "source": [
        "# Q1.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmerPYWHdZjp",
        "colab_type": "code",
        "outputId": "9e04afc3-c65b-4019-bfdc-d6380a446405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# Q1.2.a\n",
        "\n",
        "def relu( x, grad=False):\n",
        "    if grad:\n",
        "        # WRITE CODE HERE\n",
        "         return np.greater(x, 0).astype(int)\n",
        "    # WRITE CODE HERE\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "x = np.random.rand(2,3)*2 -1\n",
        "print(\"x : \",x)\n",
        "print(\"relu(x) : \", relu(x))\n",
        "print(\"relu'(x) :\", relu(x,grad = True))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x :  [[ 0.00514867 -0.09484145  0.8991575 ]\n",
            " [-0.43910339 -0.98173638  0.04652703]]\n",
            "relu(x) :  [[0.00514867 0.         0.8991575 ]\n",
            " [0.         0.         0.04652703]]\n",
            "relu'(x) : [[1 0 1]\n",
            " [0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etXNXPLMgSFm",
        "colab_type": "code",
        "outputId": "5578d546-68fe-4293-adaf-7676ac437237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "def sigmoid(x, grad=False):\n",
        "    sig = 1/(1+np.exp(-x))\n",
        "    if grad:\n",
        "        # WRITE CODE HERE\n",
        "        return sig*(1-sig)\n",
        "        # WRITE CODE HERE\n",
        "    return sig\n",
        "\n",
        "\n",
        "x = np.random.rand(2,3)*2 -1\n",
        "print(\"x : \",x)\n",
        "print(\"sigmoid(x) : \", sigmoid(x))\n",
        "print(\"sigmoid'(x) :\", sigmoid(x,grad = True))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x :  [[-0.2146626   0.77727255 -0.08572641]\n",
            " [-0.0749824   0.27609295  0.90786553]]\n",
            "sigmoid(x) :  [[0.44653948 0.68509199 0.47858151]\n",
            " [0.48126318 0.5685881  0.71256318]]\n",
            "sigmoid'(x) : [[0.24714197 0.21574096 0.24954125]\n",
            " [0.24964893 0.24529567 0.20481689]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd_X6Brwcklt",
        "colab_type": "code",
        "outputId": "9718fea1-3e2d-49e3-b43a-557d6e184176",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "def tanh( x, grad=False):\n",
        "    tanh = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "    if grad:\n",
        "        # WRITE CODE HERE\n",
        "        return 1-tanh**2\n",
        "    # WRITE CODE HERE\n",
        "    return tanh\n",
        "\n",
        "x = np.random.rand(2,3)*2 -1\n",
        "print(\"x : \",x)\n",
        "print(\"tanh(x) : \", tanh(x))\n",
        "print(\"tanh'(x) :\", tanh(x,grad = True))\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x :  [[-0.51122554  0.89105113 -0.95641943]\n",
            " [ 0.63345782 -0.18957018 -0.31317862]]\n",
            "tanh(x) :  [[-0.47089953  0.71191252 -0.74267549]\n",
            " [ 0.5604286  -0.1873315  -0.30332604]]\n",
            "tanh'(x) : [[0.77825363 0.49318057 0.44843312]\n",
            " [0.68591979 0.96490691 0.90799332]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9II2tDf1isjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def activation(self, x, grad=False):\n",
        "    if self.activation_str == \"relu\":\n",
        "        return relu(x, grad)\n",
        "    \n",
        "    elif self.activation_str == \"sigmoid\":\n",
        "        return sigmoid(x, grad)\n",
        "\n",
        "    elif self.activation_str == \"tanh\":\n",
        "        return tanh(x, grad)\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"invalid\")\n",
        "    return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWgMnONdpOlL",
        "colab_type": "text"
      },
      "source": [
        "## Q1.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKxjaYdcpRiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax( x):\n",
        "    # Remember that softmax(x-C) = softmax(x) when C is a constant.\n",
        "    # WRITE CODE HERE\n",
        "    e_x = np.exp(x.T - np.max(x, axis = -1))\n",
        "    return (e_x / e_x.sum(axis=0)).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_JtbiSnT2TI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "9dbac53d-6135-453f-a9a1-e90505b4596f"
      },
      "source": [
        "print(\"############### 2D ##############\")\n",
        "scores2D = np.array([[1, 2, 3, 6],\n",
        "                     [2, 4, 5, 6],\n",
        "                     [3, 8, 7, 6]])\n",
        "\n",
        "sfm = softmax(scores2D)\n",
        "print(sfm)\n",
        "print(np.sum(sfm[1]))\n",
        "\n",
        "print(\"input dim :\", scores2D.shape)\n",
        "print(\"softmax dim: \", sfm.shape)\n",
        "\n",
        "print(\"############### 1D ##############\")\n",
        "scores1D = np.array([1, 2, 3, 6])\n",
        "sfm1 = softmax(scores1D)\n",
        "print(sfm1)\n",
        "print(np.sum(sfm1))\n",
        "\n",
        "print(\"input dim :\", scores1D.shape)\n",
        "print(\"softmax dim: \", sfm1.shape)"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "############### 2D ##############\n",
            "[[0.00626879 0.01704033 0.04632042 0.93037047]\n",
            " [0.01203764 0.08894682 0.24178252 0.65723302]\n",
            " [0.00446236 0.66227241 0.24363641 0.08962882]]\n",
            "1.0\n",
            "input dim : (3, 4)\n",
            "softmax dim:  (3, 4)\n",
            "############### 1D ##############\n",
            "[0.00626879 0.01704033 0.04632042 0.93037047]\n",
            "1.0\n",
            "input dim : (4,)\n",
            "softmax dim:  (4,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYKsf_hQ_UQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN(object):\n",
        "    def __init__(self,\n",
        "                 hidden_dims=(512, 256),\n",
        "                 datapath='cifar10.pkl',\n",
        "                 n_classes=10,\n",
        "                 epsilon=1e-6,\n",
        "                 lr=7e-4,\n",
        "                 batch_size=1000,\n",
        "                 seed=None,\n",
        "                 activation=\"relu\",\n",
        "                 init_method=\"glorot\"\n",
        "                 ):\n",
        "\n",
        "        self.hidden_dims = hidden_dims\n",
        "        self.n_hidden = len(hidden_dims)\n",
        "        self.datapath = datapath\n",
        "        self.n_classes = n_classes\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        self.init_method = init_method\n",
        "        self.seed = seed\n",
        "        self.activation_str = activation\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        self.train_logs = {'train_accuracy': [], 'validation_accuracy': [], 'train_loss': [], 'validation_loss': []}\n",
        "\n",
        "        if datapath is not None:\n",
        "            u = pickle._Unpickler(open(datapath, 'rb'))\n",
        "            u.encoding = 'latin1'\n",
        "            self.train, self.valid, self.test = u.load()\n",
        "        else:\n",
        "            self.train, self.valid, self.test = None, None, None\n",
        "\n",
        "    # Q1.1\n",
        "    def initialize_weights(self, dims):\n",
        "        if self.seed is not None:\n",
        "            np.random.seed(self.seed)\n",
        "\n",
        "        self.weights = {}\n",
        "        # self.weights is a dictionary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers\n",
        "        all_dims = [dims[0]] + list(self.hidden_dims) + [dims[1]] ### all_dims = [input_dim, hidden1_dim, hidden2_dim, output_dim]\n",
        "        for layer_n in range(1, self.n_hidden + 2):\n",
        "            # WRITE CODE HERE #############\n",
        "            random_matrix = np.random.rand(all_dims[layer_n -1], all_dims[layer_n]) * 2 -1 ### random matrix dim: (dim_(h-1), dim_h), value range: [-1,1]\n",
        "            W = random_matrix /np.sqrt(layer_n) \n",
        "            weights[f\"W{layer_n}\"] = W\n",
        "\n",
        "            # ######################\n",
        "            self.weights[f\"b{layer_n}\"] = np.zeros((1, all_dims[layer_n])) #### row vector dim: (1, dh)\n",
        "\n",
        "    # Q1.2.a\n",
        "    def relu(self, x, grad=False):\n",
        "        if grad:\n",
        "            # WRITE CODE HERE\n",
        "            return np.greater(x, 0).astype(int)\n",
        "            # WRITE CODE HERE\n",
        "        return np.maximum(0,x)\n",
        "\n",
        "    # Q1.2.b\n",
        "    def sigmoid(self, x, grad=False):\n",
        "        sig = 1/(1+np.exp(-x))\n",
        "        if grad:\n",
        "            # WRITE CODE HERE\n",
        "            return sig*(1-sig)\n",
        "            # WRITE CODE HERE\n",
        "        return sig\n",
        "\n",
        "    # Q1.2.c\n",
        "    def tanh(self, x, grad=False):\n",
        "        tanh = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
        "        if grad:\n",
        "            # WRITE CODE HERE\n",
        "            return 1-tanh**2\n",
        "        # WRITE CODE HERE\n",
        "        return tanh\n",
        "\n",
        "    # Q1.2.d\n",
        "    def activation(self, x, grad=False):\n",
        "        if self.activation_str == \"relu\":\n",
        "            return self.relu(x, grad)\n",
        "\n",
        "        elif self.activation_str == \"sigmoid\":\n",
        "            return self.sigmoid(x, grad)\n",
        "\n",
        "        elif self.activation_str == \"tanh\":\n",
        "            return self.tanh(x, grad)\n",
        "        else:\n",
        "            raise Exception(\"invalid\")\n",
        "        return 0\n",
        "\n",
        "    # Q1.3\n",
        "    def softmax(self, x):\n",
        "        # Remember that softmax(x-C) = softmax(x) when C is a constant.\n",
        "        # WRITE CODE HERE\n",
        "        e_x = np.exp(x.T - np.max(x, axis = -1))\n",
        "        return (e_x / e_x.sum(axis=0)).T\n",
        "\n",
        "    def forward(self, x):\n",
        "        cache = {\"Z0\": x}\n",
        "        # cache is a dictionary with keys Z0, A0, ..., Zm, Am where m - 1 is the number of hidden layers\n",
        "        # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i\n",
        "        # WRITE CODE HERE\n",
        "        pass\n",
        "        return cache\n",
        "\n",
        "    def backward(self, cache, labels):\n",
        "        output = cache[f\"Z{self.n_hidden + 1}\"]\n",
        "        grads = {}\n",
        "        # grads is a dictionary with keys dAm, dWm, dbm, dZ(m-1), dA(m-1), ..., dW1, db1\n",
        "        # WRITE CODE HERE\n",
        "        pass\n",
        "        return grads\n",
        "\n",
        "    def update(self, grads):\n",
        "        for layer in range(1, self.n_hidden + 2):\n",
        "            # WRITE CODE HERE\n",
        "            pass\n",
        "\n",
        "    def one_hot(self, y):\n",
        "        # WRITE CODE HERE\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def loss(self, prediction, labels):\n",
        "        prediction[np.where(prediction < self.epsilon)] = self.epsilon\n",
        "        prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon\n",
        "        # WRITE CODE HERE\n",
        "        pass\n",
        "        return 0\n",
        "\n",
        "    def compute_loss_and_accuracy(self, X, y):\n",
        "        one_y = self.one_hot(y)\n",
        "        cache = self.forward(X)\n",
        "        predictions = np.argmax(cache[f\"Z{self.n_hidden + 1}\"], axis=1)\n",
        "        accuracy = np.mean(y == predictions)\n",
        "        loss = self.loss(cache[f\"Z{self.n_hidden + 1}\"], one_y)\n",
        "        return loss, accuracy, predictions\n",
        "\n",
        "    def train_loop(self, n_epochs):\n",
        "        X_train, y_train = self.train\n",
        "        y_onehot = self.one_hot(y_train)\n",
        "        dims = [X_train.shape[1], y_onehot.shape[1]]\n",
        "        self.initialize_weights(dims)\n",
        "\n",
        "        n_batches = int(np.ceil(X_train.shape[0] / self.batch_size))\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            for batch in range(n_batches):\n",
        "                minibatchX = X_train[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                minibatchY = y_onehot[self.batch_size * batch:self.batch_size * (batch + 1), :]\n",
        "                # WRITE CODE HERE\n",
        "                pass\n",
        "\n",
        "            X_train, y_train = self.train\n",
        "            train_loss, train_accuracy, _ = self.compute_loss_and_accuracy(X_train, y_train)\n",
        "            X_valid, y_valid = self.valid\n",
        "            valid_loss, valid_accuracy, _ = self.compute_loss_and_accuracy(X_valid, y_valid)\n",
        "\n",
        "            self.train_logs['train_accuracy'].append(train_accuracy)\n",
        "            self.train_logs['validation_accuracy'].append(valid_accuracy)\n",
        "            self.train_logs['train_loss'].append(train_loss)\n",
        "            self.train_logs['validation_loss'].append(valid_loss)\n",
        "\n",
        "        return self.train_logs\n",
        "\n",
        "    def evaluate(self):\n",
        "        X_test, y_test = self.test\n",
        "        # WRITE CODE HERE\n",
        "        pass\n",
        "        return 0"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}